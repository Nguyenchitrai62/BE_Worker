import ccxt
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import time
import os
from pymongo import MongoClient
from pymongo.server_api import ServerApi
from dotenv import load_dotenv
from scipy.signal import argrelextrema
from fastapi import FastAPI, HTTPException
from fastapi.responses import JSONResponse
import uvicorn
import pickle
import tensorflow as tf

# FastAPI app
app = FastAPI(title="Crypto Chart Pattern Analyzer API", version="1.0.0")

# Load bi·∫øn m√¥i tr∆∞·ªùng
load_dotenv()
password = os.getenv('MONGO_PASSWORD')

# K·∫øt n·ªëi MongoDB Atlas
uri = f"mongodb+srv://trainguyenchi30:{password}@cryptodata.t2i1je2.mongodb.net/?retryWrites=true&w=majority&appName=CryptoData"
client = MongoClient(uri, server_api=ServerApi('1'))

# Truy c·∫≠p DB v√† Collection
db = client['my_database']
collection = db['AI_prediction']

# K·∫øt n·ªëi v·ªõi Binance
binance = ccxt.binance({
    'apiKey': '',
    'secret': '',
})


class AIModelPredictor:
    """Class ƒë·ªÉ load v√† s·ª≠ d·ª•ng 2 AI models: long v√† short"""
    
    def __init__(self, model_configs=None):
        """
        model_configs: dict v·ªõi format:
        {
            'long': 'models/best_long_model.h5',
            'short': 'models/best_short_model.h5'
        }
        """
        if model_configs is None:
            model_configs = {
                'long': 'models/best_long_model.h5',
                'short': 'models/best_short_model.h5'
            }
        
        self.model_configs = model_configs
        self.models = {}
        self.scalers_X = {}
        self.scalers_y = {}
        self.model_infos = {}
        self.load_all_models()
    
    def load_model_by_type(self, model_type, model_path):
        """Load m·ªôt model c·ª• th·ªÉ theo type (long ho·∫∑c short)"""
        try:
            print(f"üì• Loading {model_type} model from {model_path}...")
            
            # Load model
            model = tf.keras.models.load_model(model_path)
            
            # Load scalers v√† model info
            scaler_X_path = f'models/{model_type}_scaler_X.pkl'
            scaler_y_path = f'models/{model_type}_scaler_y.pkl'
            model_info_path = f'models/{model_type}_model_info.pkl'
            
            with open(scaler_X_path, 'rb') as f:
                scaler_X = pickle.load(f)
            
            with open(scaler_y_path, 'rb') as f:
                scaler_y = pickle.load(f)
            
            with open(model_info_path, 'rb') as f:
                model_info = pickle.load(f)
            
            # L∆∞u v√†o dictionaries
            self.models[model_type] = model
            self.scalers_X[model_type] = scaler_X
            self.scalers_y[model_type] = scaler_y
            self.model_infos[model_type] = model_info
            
            print(f"‚úÖ {model_type.capitalize()} model loaded successfully!")
            print(f"üìä {model_type} info: R¬≤ = {model_info['r2_score']:.4f}, RMSE = {model_info['rmse']:.6f}")
            
            return True
            
        except Exception as e:
            print(f"‚ùå Error loading {model_type} model: {e}")
            return False
    
    def load_all_models(self):
        """Load c·∫£ 2 models: long v√† short"""
        success_count = 0
        for model_type, model_path in self.model_configs.items():
            if self.load_model_by_type(model_type, model_path):
                success_count += 1
        
        print(f"üéØ Successfully loaded {success_count}/{len(self.model_configs)} models")
    
    def prepare_features(self, df):
        """Chu·∫©n b·ªã features gi·ªëng nh∆∞ trong training"""
        df_copy = df.copy()
        
        # T√≠nh c√°c ch·ªâ b√°o (gi·ªëng nh∆∞ trong training)
        df_copy['ma7'] = df_copy['Close'].rolling(window=7).mean()
        df_copy['price_vs_ma7'] = (df_copy['Close'] - df_copy['ma7']) / df_copy['ma7']
        df_copy['close/open'] = df_copy['Close'] / df_copy['Open'] - 1
        
        # Lo·∫°i b·ªè NaN
        df_copy = df_copy.dropna(subset=['price_vs_ma7', 'close/open'])
        
        return df_copy
    
    def predict_max_gain_pct_single(self, sequence, features, model_type='long'):
        """D·ª± ƒëo√°n max_gain_pct cho m·ªôt sequence v·ªõi model c·ª• th·ªÉ"""
        if model_type not in self.models:
            print(f"‚ùå Model {model_type} not available")
            return None
        
        try:
            model = self.models[model_type]
            scaler_X = self.scalers_X[model_type]
            scaler_y = self.scalers_y[model_type]
            
            # Chu·∫©n h√≥a sequence
            seq_scaled = scaler_X.transform(sequence.reshape(-1, len(features)))
            seq_scaled = seq_scaled.reshape(1, len(sequence), len(features))
            
            # Predict scaled value
            pred_scaled = model.predict(seq_scaled, verbose=0)[0][0]
            
            # Convert back to original scale
            pred_original = scaler_y.inverse_transform([[pred_scaled]])[0][0]
            
            return pred_original
        except Exception as e:
            print(f"‚ö†Ô∏è Error in {model_type} prediction: {e}")
            return None
    
    def predict_for_dataframe(self, df):
        """
        Predict cho to√†n b·ªô DataFrame:
        - signal = 1: s·ª≠ d·ª•ng long model
        - signal = 0: s·ª≠ d·ª•ng short model
        K·∫øt qu·∫£ l∆∞u v√†o 1 c·ªôt predicted_target duy nh·∫•t
        """
        if 'long' not in self.models or 'short' not in self.models:
            print("‚ùå Both long and short models must be loaded")
            return df
        
        try:
            # Chu·∫©n b·ªã features
            df_prepared = self.prepare_features(df)
            
            # Th√™m c·ªôt predicted_target
            df_prepared['predicted_target'] = np.nan
            
            # L·∫•y th√¥ng tin t·ª´ c·∫£ 2 models
            long_info = self.model_infos['long']
            short_info = self.model_infos['short']
            
            long_sequence_len = long_info['sequence_len']
            short_sequence_len = short_info['sequence_len']
            long_features = long_info['features']
            short_features = short_info['features']
            
            print(f"üìã Long model - sequence length: {long_sequence_len}, features: {long_features}")
            print(f"üìã Short model - sequence length: {short_sequence_len}, features: {short_features}")
            
            long_predictions = 0
            short_predictions = 0
            
            # T√≠nh max sequence length ƒë·ªÉ ƒë·∫£m b·∫£o c√≥ ƒë·ªß data
            max_sequence_len = max(long_sequence_len, short_sequence_len)
            
            # Duy·ªát qua t·∫•t c·∫£ c√°c ƒëi·ªÉm
            for i in range(max_sequence_len, len(df_prepared)):
                signal_value = df_prepared['signal'].iloc[i]
                
                if signal_value == 1:
                    # S·ª≠ d·ª•ng long model
                    if i >= long_sequence_len:
                        seq = df_prepared[long_features].iloc[i-long_sequence_len:i].values
                        predicted_gain = self.predict_max_gain_pct_single(seq, long_features, 'long')
                        
                        if predicted_gain is not None:
                            df_prepared.loc[df_prepared.index[i], 'predicted_target'] = predicted_gain
                            long_predictions += 1
                
                elif signal_value == 0:
                    # S·ª≠ d·ª•ng short model
                    if i >= short_sequence_len:
                        seq = df_prepared[short_features].iloc[i-short_sequence_len:i].values
                        predicted_gain = self.predict_max_gain_pct_single(seq, short_features, 'short')
                        
                        if predicted_gain is not None:
                            df_prepared.loc[df_prepared.index[i], 'predicted_target'] = predicted_gain
                            short_predictions += 1
            
            total_predictions = long_predictions + short_predictions
            print(f"üéØ Made {long_predictions} predictions with long model (signal=1)")
            print(f"üéØ Made {short_predictions} predictions with short model (signal=0)")
            print(f"üéØ Total predictions made: {total_predictions}")
            
            return df_prepared
            
        except Exception as e:
            print(f"‚ùå Error in AI prediction process: {e}")
            return df
    
    def get_model_info(self, model_type=None):
        """L·∫•y th√¥ng tin v·ªÅ model(s)"""
        if model_type is None:
            return self.model_infos
        else:
            return self.model_infos.get(model_type, None)
    
    def get_available_models(self):
        """L·∫•y danh s√°ch c√°c model ƒë√£ load th√†nh c√¥ng"""
        return list(self.models.keys())

class RealtimeCryptoPatternAnalyzer:
    def __init__(self):
        self.df = None
        
        # ƒê·ªânh/ƒë√°y order=1 (realtime)
        self.realtime_highs = None
        self.realtime_lows = None
        
        # ƒê·ªânh/ƒë√°y order=5 (ƒë√°ng tin c·∫≠y)
        self.reliable_highs = None
        self.reliable_lows = None
        
        # AI Model Predictor
        self.ai_predictor = AIModelPredictor()
    
    def fetch_data(self, symbol='BTC/USDT', timeframe='1h', count=1000):
        """L·∫•y d·ªØ li·ªáu OHLCV t·ª´ Binance"""
        try:
            ohlcv = binance.fetch_ohlcv(symbol, timeframe=timeframe, limit=count)
            self.df = pd.DataFrame(ohlcv, columns=['Date', 'Open', 'High', 'Low', 'Close', 'Volume'])
            self.df['Date'] = pd.to_datetime(self.df['Date'], unit='ms')
            self.df['signal'] = -1  # M·∫∑c ƒë·ªãnh signal l√† -1
            return self.df
        except Exception as e:
            raise Exception(f"Error fetching data: {e}")
    
    def find_peaks_and_troughs(self, order_realtime=1, order_reliable=20):
        """
        T√¨m ƒë·ªânh/ƒë√°y v·ªõi 2 m·ª©c ƒë·ªô tin c·∫≠y:
        - order_realtime: Cho trading realtime (th∆∞·ªùng l√† 1)
        - order_reliable: Cho c√°c ƒë·ªânh/ƒë√°y ƒë√°ng tin c·∫≠y (th∆∞·ªùng l√† 5)
        """
        if self.df is None or len(self.df) == 0:
            raise ValueError("C·∫ßn fetch d·ªØ li·ªáu tr∆∞·ªõc")
            
        # ƒê·ªânh/ƒë√°y realtime (order=1)
        self.realtime_highs = argrelextrema(self.df['Close'].values, np.greater, order=order_realtime)[0]
        self.realtime_lows = argrelextrema(self.df['Close'].values, np.less, order=order_realtime)[0]
        
        # ƒê·ªânh/ƒë√°y ƒë√°ng tin c·∫≠y (order=5)
        self.reliable_highs = argrelextrema(self.df['Close'].values, np.greater, order=order_reliable)[0]
        self.reliable_lows = argrelextrema(self.df['Close'].values, np.less, order=order_reliable)[0]
        
        return {
            'realtime_highs': self.realtime_highs,
            'realtime_lows': self.realtime_lows,
            'reliable_highs': self.reliable_highs,
            'reliable_lows': self.reliable_lows
        }
    
    def detect_realtime_double_top(self, threshold=0.02, lookback_window=50):
        """
        Ph√°t hi·ªán Double Top v·ªõi logic th·ª±c t·∫ø:
        - S·ª≠ d·ª•ng ƒë·ªânh realtime (order=1) hi·ªán t·∫°i
        - So s√°nh v·ªõi ƒë·ªânh reliable (order=5) tr∆∞·ªõc ƒë√≥ trong kho·∫£ng lookback_window
        """
        if self.realtime_highs is None or self.reliable_highs is None:
            raise ValueError("C·∫ßn ch·∫°y find_peaks_and_troughs() tr∆∞·ªõc")
        
        signals = np.full(len(self.df), -1)
        
        # Duy·ªát qua c√°c ƒë·ªânh realtime
        for current_high_idx in self.realtime_highs:
            current_high_price = self.df['High'].iloc[current_high_idx]
            
            # T√¨m ƒë·ªânh reliable tr∆∞·ªõc ƒë√≥ trong kho·∫£ng lookback_window
            reliable_highs_before = [
                idx for idx in self.reliable_highs 
                if current_high_idx - lookback_window <= idx < current_high_idx
            ]
            
            if not reliable_highs_before:
                continue
            
            # T√¨m ƒë·ªânh reliable g·∫ßn nh·∫•t v√† cao nh·∫•t
            for reliable_high_idx in reliable_highs_before:
                reliable_high_price = self.df['High'].iloc[reliable_high_idx]
                
                # Ki·ªÉm tra ƒëi·ªÅu ki·ªán Double Top
                price_diff = abs(current_high_price - reliable_high_price) / reliable_high_price
                
                if price_diff <= threshold:
                    # Ki·ªÉm tra c√≥ ƒë√°y ·ªü gi·ªØa kh√¥ng (ƒë·ªÉ x√°c nh·∫≠n l√† double top)
                    valley_between = self.find_valley_between(reliable_high_idx, current_high_idx)
                    
                    if valley_between is not None:
                        # T√≠n hi·ªáu b√°n xu·∫•t hi·ªán ngay sau ƒë·ªânh hi·ªán t·∫°i
                        if current_high_idx + 1 < len(self.df):
                            signals[current_high_idx + 1] = 0  # T√≠n hi·ªáu b√°n
                        break
        
        return signals
    
    def detect_realtime_double_bottom(self, threshold=0.02, lookback_window=50):
        """
        Ph√°t hi·ªán Double Bottom v·ªõi logic th·ª±c t·∫ø:
        - S·ª≠ d·ª•ng ƒë√°y realtime (order=1) hi·ªán t·∫°i
        - So s√°nh v·ªõi ƒë√°y reliable (order=5) tr∆∞·ªõc ƒë√≥ trong kho·∫£ng lookback_window
        """
        if self.realtime_lows is None or self.reliable_lows is None:
            raise ValueError("C·∫ßn ch·∫°y find_peaks_and_troughs() tr∆∞·ªõc")
        
        signals = np.full(len(self.df), -1)
        
        # Duy·ªát qua c√°c ƒë√°y realtime
        for current_low_idx in self.realtime_lows:
            current_low_price = self.df['Low'].iloc[current_low_idx]
            
            # T√¨m ƒë√°y reliable tr∆∞·ªõc ƒë√≥ trong kho·∫£ng lookback_window
            reliable_lows_before = [
                idx for idx in self.reliable_lows 
                if current_low_idx - lookback_window <= idx < current_low_idx
            ]
            
            if not reliable_lows_before:
                continue
            
            # T√¨m ƒë√°y reliable g·∫ßn nh·∫•t v√† th·∫•p nh·∫•t
            for reliable_low_idx in reliable_lows_before:
                reliable_low_price = self.df['Low'].iloc[reliable_low_idx]
                
                # Ki·ªÉm tra ƒëi·ªÅu ki·ªán Double Bottom
                price_diff = abs(current_low_price - reliable_low_price) / reliable_low_price
                
                if price_diff <= threshold:
                    # Ki·ªÉm tra c√≥ ƒë·ªânh ·ªü gi·ªØa kh√¥ng (ƒë·ªÉ x√°c nh·∫≠n l√† double bottom)
                    peak_between = self.find_peak_between(reliable_low_idx, current_low_idx)
                    
                    if peak_between is not None:
                        # T√≠n hi·ªáu mua xu·∫•t hi·ªán ngay sau ƒë√°y hi·ªán t·∫°i
                        if current_low_idx + 1 < len(self.df):
                            signals[current_low_idx + 1] = 1  # T√≠n hi·ªáu mua
                        break
        
        return signals
    
    def find_valley_between(self, start_idx, end_idx):
        """T√¨m ƒë√°y th·∫•p nh·∫•t gi·ªØa hai ƒëi·ªÉm"""
        if start_idx >= end_idx:
            return None
        
        valley_section = self.df['Low'].iloc[start_idx:end_idx+1]
        if len(valley_section) == 0:
            return None
        
        min_idx = valley_section.idxmin()
        return min_idx
    
    def find_peak_between(self, start_idx, end_idx):
        """T√¨m ƒë·ªânh cao nh·∫•t gi·ªØa hai ƒëi·ªÉm"""
        if start_idx >= end_idx:
            return None
        
        peak_section = self.df['High'].iloc[start_idx:end_idx+1]
        if len(peak_section) == 0:
            return None
        
        max_idx = peak_section.idxmax()
        return max_idx
    
    def detect_realtime_head_and_shoulders(self, threshold=0.02, lookback_window=100):
        """
        Ph√°t hi·ªán Head and Shoulders v·ªõi logic th·ª±c t·∫ø:
        - S·ª≠ d·ª•ng ƒë·ªânh realtime hi·ªán t·∫°i l√†m right shoulder
        - T√¨m head v√† left shoulder t·ª´ c√°c ƒë·ªânh reliable tr∆∞·ªõc ƒë√≥
        """
        if self.realtime_highs is None or self.reliable_highs is None:
            raise ValueError("C·∫ßn ch·∫°y find_peaks_and_troughs() tr∆∞·ªõc")
        
        signals = np.full(len(self.df), -1)
        
        # Duy·ªát qua c√°c ƒë·ªânh realtime l√†m right shoulder
        for right_shoulder_idx in self.realtime_highs:
            right_shoulder_price = self.df['High'].iloc[right_shoulder_idx]
            
            # T√¨m c√°c ƒë·ªânh reliable tr∆∞·ªõc ƒë√≥ ƒë·ªÉ l√†m head v√† left shoulder
            reliable_highs_before = [
                idx for idx in self.reliable_highs 
                if right_shoulder_idx - lookback_window <= idx < right_shoulder_idx
            ]
            
            if len(reliable_highs_before) < 2:
                continue
            
            # Th·ª≠ c√°c k·∫øt h·ª£p head v√† left shoulder
            for i in range(len(reliable_highs_before) - 1):
                for j in range(i + 1, len(reliable_highs_before)):
                    left_shoulder_idx = reliable_highs_before[i]
                    head_idx = reliable_highs_before[j]
                    
                    left_shoulder_price = self.df['High'].iloc[left_shoulder_idx]
                    head_price = self.df['High'].iloc[head_idx]
                    
                    # Ki·ªÉm tra ƒëi·ªÅu ki·ªán H&S
                    if (head_price > left_shoulder_price and 
                        head_price > right_shoulder_price and
                        abs(left_shoulder_price - right_shoulder_price) / head_price <= threshold):
                        
                        # T√¨m neckline
                        neck_left = self.find_valley_between(left_shoulder_idx, head_idx)
                        neck_right = self.find_valley_between(head_idx, right_shoulder_idx)
                        
                        if neck_left is not None and neck_right is not None:
                            # T√≠n hi·ªáu b√°n xu·∫•t hi·ªán ngay sau right shoulder
                            if right_shoulder_idx + 1 < len(self.df):
                                signals[right_shoulder_idx + 1] = 0  # T√≠n hi·ªáu b√°n
                            break
                
                if signals[right_shoulder_idx + 1] == 0:  # ƒê√£ t√¨m th·∫•y pattern
                    break
        
        return signals
    
    def detect_realtime_inverted_head_and_shoulders(self, threshold=0.02, lookback_window=100):
        """
        Ph√°t hi·ªán Inverted Head and Shoulders v·ªõi logic th·ª±c t·∫ø:
        - S·ª≠ d·ª•ng ƒë√°y realtime hi·ªán t·∫°i l√†m right shoulder
        - T√¨m head v√† left shoulder t·ª´ c√°c ƒë√°y reliable tr∆∞·ªõc ƒë√≥
        """
        if self.realtime_lows is None or self.reliable_lows is None:
            raise ValueError("C·∫ßn ch·∫°y find_peaks_and_troughs() tr∆∞·ªõc")
        
        signals = np.full(len(self.df), -1)
        
        # Duy·ªát qua c√°c ƒë√°y realtime l√†m right shoulder
        for right_shoulder_idx in self.realtime_lows:
            right_shoulder_price = self.df['Low'].iloc[right_shoulder_idx]
            
            # T√¨m c√°c ƒë√°y reliable tr∆∞·ªõc ƒë√≥ ƒë·ªÉ l√†m head v√† left shoulder
            reliable_lows_before = [
                idx for idx in self.reliable_lows 
                if right_shoulder_idx - lookback_window <= idx < right_shoulder_idx
            ]
            
            if len(reliable_lows_before) < 2:
                continue
            
            # Th·ª≠ c√°c k·∫øt h·ª£p head v√† left shoulder
            for i in range(len(reliable_lows_before) - 1):
                for j in range(i + 1, len(reliable_lows_before)):
                    left_shoulder_idx = reliable_lows_before[i]
                    head_idx = reliable_lows_before[j]
                    
                    left_shoulder_price = self.df['Low'].iloc[left_shoulder_idx]
                    head_price = self.df['Low'].iloc[head_idx]
                    
                    # Ki·ªÉm tra ƒëi·ªÅu ki·ªán Inverted H&S
                    if (head_price < left_shoulder_price and 
                        head_price < right_shoulder_price and
                        abs(left_shoulder_price - right_shoulder_price) / head_price <= threshold):
                        
                        # T√¨m neckline
                        neck_left = self.find_peak_between(left_shoulder_idx, head_idx)
                        neck_right = self.find_peak_between(head_idx, right_shoulder_idx)
                        
                        if neck_left is not None and neck_right is not None:
                            # T√≠n hi·ªáu mua xu·∫•t hi·ªán ngay sau right shoulder
                            if right_shoulder_idx + 1 < len(self.df):
                                signals[right_shoulder_idx + 1] = 1  # T√≠n hi·ªáu mua
                            break
                
                if signals[right_shoulder_idx + 1] == 1:  # ƒê√£ t√¨m th·∫•y pattern
                    break
        
        return signals
    
    def combine_signals(self, *signal_arrays):
        """K·∫øt h·ª£p nhi·ªÅu m·∫£ng t√≠n hi·ªáu"""
        for signals in signal_arrays:
            self.df['signal'] = np.where(signals != -1, signals, self.df['signal'])
    
    def save_to_mongodb(self, symbol='BTC/USDT', timeframe='1h'):
        """L∆∞u k·∫øt qu·∫£ ph√¢n t√≠ch v√†o MongoDB"""
        try:
            # Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·ªÉ l∆∞u
            data_to_save = []
            
            for index, row in self.df.iterrows():
                record = {
                    'Date': row['Date'],
                    'Open': float(row['Open']),
                    'High': float(row['High']),
                    'Low': float(row['Low']),
                    'Close': float(row['Close']),
                    'confidence': int(row['signal']),
                    'symbol': symbol
                }
                
                # Th√™m predicted_target n·∫øu symbol l√† BTC/USDT v√† c√≥ d·ªØ li·ªáu prediction
                if symbol == 'BTC/USDT' and 'predicted_target' in self.df.columns:
                    if pd.notna(row['predicted_target']):
                        record['predicted_target'] = float(row['predicted_target'])
                    else:
                        record['predicted_target'] = None  # L∆∞u None thay v√¨ b·ªè qua
                
                data_to_save.append(record)
            
            # L∆∞u v√†o MongoDB
            if data_to_save:
                # X√≥a d·ªØ li·ªáu c≈© c·ªßa symbol n√†y
                delete_result = collection.delete_many({'symbol': symbol})
                print(f"üóëÔ∏è Deleted {delete_result.deleted_count} old records for {symbol}")

                # Ch√®n d·ªØ li·ªáu m·ªõi
                result = collection.insert_many(data_to_save)
                
                # Th·ªëng k√™ cho BTC
                if symbol == 'BTC/USDT':
                    predicted_count = sum(1 for record in data_to_save if record.get('predicted_target') is not None)
                    print(f"üìä BTC: Inserted {len(result.inserted_ids)} records, {predicted_count} with AI predictions")
                else:
                    print(f"üìä {symbol}: Inserted {len(result.inserted_ids)} records")
                
                return len(result.inserted_ids)
            else:
                return 0
                
        except Exception as e:
            print(f"‚ùå Error saving {symbol} to MongoDB: {e}")
            raise Exception(f"Error saving to MongoDB: {e}")
    
    def run_analysis(self, symbol='BTC/USDT', timeframe='1h', count=1000, order_realtime=1, order_reliable=20, threshold=0.01, lookback_window=10):
        """Ch·∫°y ph√¢n t√≠ch v√† l∆∞u v√†o MongoDB"""

        print(f"üöÄ Starting analysis for {symbol}...")
        
        # B∆∞·ªõc 1: Crawl d·ªØ li·ªáu
        self.fetch_data(symbol, timeframe, count)
        print(f"üìä Fetched {len(self.df)} records for {symbol}")

        # B∆∞·ªõc 2: T√¨m ƒë·ªânh v√† ƒë√°y
        peaks_troughs = self.find_peaks_and_troughs(order_realtime=order_realtime, order_reliable=order_reliable)
        print(f"üîç Found peaks/troughs: Realtime highs={len(peaks_troughs['realtime_highs'])}, Realtime lows={len(peaks_troughs['realtime_lows'])}")

        # B∆∞·ªõc 3: Nh·∫≠n di·ªán c√°c m√¥ h√¨nh
        dt_signals = self.detect_realtime_double_top(threshold=threshold, lookback_window=lookback_window)
        db_signals = self.detect_realtime_double_bottom(threshold=threshold, lookback_window=lookback_window)
        hs_signals = self.detect_realtime_head_and_shoulders(threshold=threshold, lookback_window=lookback_window)
        ihs_signals = self.detect_realtime_inverted_head_and_shoulders(threshold=threshold, lookback_window=lookback_window)

        # B∆∞·ªõc 4: K·∫øt h·ª£p t√≠n hi·ªáu
        self.combine_signals(dt_signals, db_signals, hs_signals, ihs_signals)
        
        # ƒê·∫øm s·ªë t√≠n hi·ªáu
        buy_signals = (self.df['signal'] == 1).sum()
        sell_signals = (self.df['signal'] == 0).sum()
        print(f"üìà Chart patterns: {buy_signals} buy signals, {sell_signals} sell signals")

        # B∆∞·ªõc 5: Ch·∫°y AI prediction CH·ªà cho BTC (model ƒë∆∞·ª£c train ri√™ng cho BTC)
        if symbol == 'BTC/USDT':
            print("ü§ñ Running AI predictions for BTC...")
            self.df = self.ai_predictor.predict_for_dataframe(self.df)
            
            # Ki·ªÉm tra k·∫øt qu·∫£ AI prediction
            if 'predicted_target' in self.df.columns:
                pred_count = self.df['predicted_target'].notna().sum()
                print(f"üéØ AI Predictions: {pred_count} predictions made")
                
                # In m·ªôt v√†i sample predictions ƒë·ªÉ debug
                sample_predictions = self.df[self.df['predicted_target'].notna()][['Date', 'Close', 'signal', 'predicted_target']].tail(5)
                if len(sample_predictions) > 0:
                    print("üìã Sample predictions:")
                    for _, row in sample_predictions.iterrows():
                        print(f"   {row['Date']}: Close={row['Close']:.2f}, Signal={row['signal']}, Predicted={row['predicted_target']:.4f}")
            else:
                print("‚ö†Ô∏è No predicted_target column found after AI prediction")
        else:
            print(f"‚è≠Ô∏è Skipping AI predictions for {symbol} (model only works for BTC)")

        # B∆∞·ªõc 6: L∆∞u v√†o MongoDB
        saved_count = self.save_to_mongodb(symbol, timeframe)
        
        return saved_count


# ===============================
# FASTAPI ENDPOINTS
# ===============================

@app.get("/")
async def root():
    """Root endpoint"""
    return {
        "message": "Crypto Chart Pattern Analyzer API with AI Predictions",
        "version": "2.0.0",
        "status": "running",
        "features": ["Chart Pattern Detection", "AI Price Prediction for BTC"]
    }

@app.get("/ping")
async def ping():
    """
    Endpoint ƒë·ªÉ c·∫≠p nh·∫≠t d·ªØ li·ªáu m·ªõi nh·∫•t v√†o MongoDB
    V·ªõi AI predictions cho BTC
    """
    try:
        results = {}
        
        # Danh s√°ch symbols ƒë·ªÉ analyze
        symbols = ['BTC/USDT', 'ETH/USDT', 'XRP/USDT', 'SOL/USDT']
        
        for symbol in symbols:
            print(f"\n{'='*50}")
            print(f"Processing {symbol}")
            print(f"{'='*50}")
            
            # T·∫°o analyzer m·ªõi cho m·ªói symbol
            analyzer = RealtimeCryptoPatternAnalyzer()
            
            # Ch·∫°y ph√¢n t√≠ch
            analyzer.run_analysis(symbol='BTC/USDT', timeframe='1h', order_realtime=1, order_reliable=20, threshold=0.01, lookback_window=10)
            analyzer.run_analysis(symbol='ETH/USDT', timeframe='1h', order_realtime=1, order_reliable=20, threshold=0.02, lookback_window=10)
            analyzer.run_analysis(symbol='SOL/USDT', timeframe='1h', order_realtime=1, order_reliable=20, threshold=0.02, lookback_window=10)
            analyzer.run_analysis(symbol='XRP/USDT', timeframe='1h', order_realtime=1, order_reliable=20, threshold=0.015, lookback_window=10)
            
        return JSONResponse(
            status_code=200,
            content={
                "message": "DONE",
            }
        )
            
    except Exception as e:
        print(f"‚ùå Error in ping endpoint: {e}")
        raise HTTPException(
            status_code=500,
            detail={
                "message": "ERROR",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
        )

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    try:
        # Test MongoDB connection
        client.admin.command('ping')
        
        # Test AI model status
        temp_analyzer = RealtimeCryptoPatternAnalyzer()
        model_status = temp_analyzer.ai_predictor.model is not None
        
        return JSONResponse(
            status_code=200,
            content={
                "status": "healthy",
                "mongodb": "connected",
                "ai_model": "loaded" if model_status else "not_loaded",
                "timestamp": datetime.now().isoformat()
            }
        )
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail={
                "status": "unhealthy",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
        )


def start_api_server(host="0.0.0.0", port=8000):
    """Kh·ªüi ƒë·ªông FastAPI server"""
    print(f"üöÄ Starting FastAPI server with AI Integration on {host}:{port}")
    print(f"üìñ API Documentation: http://{host}:{port}/docs")
    print(f"üîç Endpoints:")
    print(f"   - GET /ping - Update latest data to MongoDB (with AI predictions for BTC)")
    print(f"   - GET /health - Health check")
    print(f"ü§ñ AI Model: Enabled for BTC/USDT predictions")
    print("-" * 50)
    
    uvicorn.run(app, host=host, port=port)


if __name__ == "__main__":
    start_api_server()